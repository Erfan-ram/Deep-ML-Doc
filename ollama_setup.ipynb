{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erfan-ram/Deep-ML-Doc/blob/main/ollama_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Ollama in Colab\n",
        "---\n",
        "\n",
        "[![5aharsh/collama](https://raw.githubusercontent.com/5aharsh/collama/main/assets/banner.png)](https://github.com/5aharsh/collama)\n",
        "\n",
        "This is an example notebook which demonstrates how to run Ollama inside a Colab instance. With this you can run pretty much any small to medium sized models offerred by Ollama for free.\n",
        "\n",
        "For the list of available models check [models being offerred by Ollama](https://ollama.com/library).\n",
        "\n",
        "\n",
        "## Before you proceed\n",
        "---\n",
        "\n",
        "Since by default the runtime type of Colab instance is CPU based, in order to use LLM models make sure to change your runtime type to T4 GPU (or better if you're a paid Colab user). This can be done by going to **Runtime > Change runtime type**.\n",
        "\n",
        "While running your script be mindful of the resources you're using. This can be tracked at **Runtime > View resources**.\n",
        "\n",
        "## Running the notebook\n",
        "---\n",
        "\n",
        "After configuring the runtime just run it with **Runtime > Run all**. And you can start tinkering around. This example uses [Llama 3.2](https://ollama.com/library/llama3.2) to generate a response from a prompted question using [LangChain Ollama Integration](https://python.langchain.com/docs/integrations/chat/ollama/)."
      ],
      "metadata": {
        "id": "zyGk-87qnbWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies\n",
        "---\n",
        "\n",
        "1. `pciutils` is required by Ollama to detect the GPU type.\n",
        "2. Installation of Ollama in the runtime instance will be taken care by `curl -fsSL https://ollama.com/install.sh | sh`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B1S1YL6EnYBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "YlVK9iG4AD5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda5eea9-c18a-4e5c-e1cf-b42fafc28c08"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,688 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,099 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,824 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Fetched 20.5 MB in 3s (6,511 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 pci.ids all 0.0~2022.01.22-1ubuntu0.1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 1s (572 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 126315 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Ollama\n",
        "---\n",
        "\n",
        "In order to use Ollama it needs to run as a service in background parallel to your scripts. Becasue Jupyter Notebooks is built to run code blocks in sequence this make it difficult to run two blocks at the same time. As a workaround we will create a service using subprocess in Python so it doesn't block any cell from running.\n",
        "\n",
        "Service can be started by command `ollama serve`.\n",
        "\n",
        "`time.sleep(5)` adds some delay to get the Ollama service up before downloading the model."
      ],
      "metadata": {
        "id": "fGEJwjTPoKWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "Jh5CBAFxBYAC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pulling Model\n",
        "---\n",
        "\n",
        "Download the LLM model using `ollama pull llama3.2`.\n",
        "\n",
        "For other models check https://ollama.com/library"
      ],
      "metadata": {
        "id": "WcBLqZfyoHg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2:1b"
      ],
      "metadata": {
        "id": "pQs2nYZvOz4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2"
      ],
      "metadata": {
        "id": "xzLsN2x8Uxl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull deepseek-r1:8b\n"
      ],
      "metadata": {
        "id": "o2ghppmRDFny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.1:8b\n",
        "\n"
      ],
      "metadata": {
        "id": "VXJoQ9v1jMs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull mistral"
      ],
      "metadata": {
        "id": "IBZEj0hKbFi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull phi4"
      ],
      "metadata": {
        "id": "0jDcVE-vYzs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3:4b"
      ],
      "metadata": {
        "id": "kbBJNFIJo3bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And that's it!\n",
        "---\n",
        "\n",
        "With this you should be able to freely play around with the models in your scripts. Following is an example using `langchain-ollama` to answer a simple prompt.\n",
        "\n",
        "If you have a use-case that can help out others feel free to add your notebook to [Collama](https://github.com/5aharsh/collama/fork)"
      ],
      "metadata": {
        "id": "TYQJNeTuni_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-ollama"
      ],
      "metadata": {
        "id": "MbrT39oil6tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from IPython.display import Markdown\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# model = OllamaLLM(model=\"deepseek-r1:8b\")\n",
        "# model = OllamaLLM(model=\"llama3.1:8b\")\n",
        "# model = OllamaLLM(model=\"gemma3:4b\")\n",
        "# model = OllamaLLM(model=\"mistral\")\n",
        "\n",
        "\n",
        "# model = OllamaLLM(model=\"llama3.2\")\n",
        "# https://www.youtube.com/watch?v=3tkmnItNXJM\n",
        "\n",
        "# model = OllamaLLM(model=\"deepseek-r1:7b\")\n",
        "# model = OllamaLLM(model=\"phi4\")\n",
        "\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "# from langchain.llms import OllamaLLM  # Adjust the import if necessary\n",
        "\n",
        "# Set up the model with streaming enabled\n",
        "model = OllamaLLM(\n",
        "    model=\"gemma3:4b\",\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "# display(Markdown(chain.invoke({\"question\": \"hi could you generate me a story about a person whos name is yosef and he live in iran . he is 21 years old and he study computer enginearing\"})))\n",
        "chain.invoke({\"question\": \"یک داستان در مورد یک پسر 15 ساله که در تهران زندگی میکند بهم بگو که باشد به زبان فارسی\"})\n",
        "chain.invoke({\"question\": \"خداوند کیست ؟ به زبان فارسی\"})\n",
        "# display(Markdown(chain.invoke({\"question\": \"تهران در کجا قرار دارد ؟ . کمی درباره تهران بهم بگو. شیعه چیست . 10 خط درباره آن توضیح بده\"})))\n"
      ],
      "metadata": {
        "id": "9quBP56zDvpt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "outputId": "9ebe691f-5d85-4cb2-9df4-937bb29ce925"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let’s craft a story about a 15-year-old boy living in Tehran. Here's a draft, aiming for a natural and engaging tone:\n",
            "\n",
            "---\n",
            "\n",
            "**عنوان: دود و ستاره‌ها**\n",
            "\n",
            "آرش، پانزده سالش بود و زندگی‌اش مثل خیابون‌های شلوغ و پر دود تهران، پر از صدا و اتفاق بود. خونه‌اش تو محله‌ای قدیمی‌تر، نزدیک بازارِ قیچی، بود. بوی ادویه، روغن زیتون و قهوه، همه‌شون توی خونش پیچیده بود و با بوی بارونِ شب، یه جور حس خاصی بهش می‌داد.\n",
            "\n",
            "آرش بیشتر وقت‌‌ها تو کافه کنار خونه‌اش، \"قهوه دلتنگی\"، می‌خوند یا با دوستاش حرف می‌زد. دوستاش هم مثل خودش، بچه‌هاییِ بی‌خیال و سرگردون، بودند که روی مسائلِ زندگی به seriousness فکر نمی‌کردند. بعضی‌شون تو مدرسه فوتبال بازی می‌کردن، بعضی‌‌ها تو نقاشی وقت می‌گذروندن و بعضی‌ها هم فقط دنبالِ یه کم تفریح و خوش گذرونی بودند.\n",
            "\n",
            "اما آرش، یه چیزی تو دلش داشت که نمی‌دونست چیه. یه حسِ شکسته بودن، یه چیزی که داشت اون رو به سمت یه چیزیِ دیگه می‌کشید. این حس، مخصوصا وقتی که به آسمونِ تاریکِ تهران نگاه می‌کرد، قوی‌تر می‌شد.  از پشتِ دودِ بلند و ستون‌های بلندِ شهر، ستاره‌ها می‌زدن در رو بهش.  یه جورایی، اون فکر می‌کرد این ستاره‌ها دارن بهش می‌گنن یه کاری براش انجام بده، یه چیزی که بتونه یه ذره به این شهر و به خودش حسِ بهتری بده.\n",
            "\n",
            "یه روز، توی کافه، با یه پیرمردِ ساده، به اسم آقای رضایی آشنا شد. آقای رضایی، یه نویسنده بود که بیشتر وقتش رو توی پارکِ لاله می‌گذروند و داستان می‌نوشت. آقای رضایی به آرش گفت: \"زندگی مثل یه داستانه، پسرم. تو می‌تونی خودت نویسنده زندگی‌ات باشی.\"\n",
            "\n",
            "آرش، شروع کرد به فکر کردن. شاید آقای رضایی درست می‌گفت. شاید اون می‌تونه یه کاری بکنه، یه چیزی که بتونه به این شهر، به این زندگی، یه جور حسِ امید بده.\n",
            "\n",
            "او شروع کرد به جمع‌آوری زباله‌های اطرافش، با بچه‌های دیگه، یه پارک کوچیک تو محله‌شون درست کردند و به فکر کمک به نیازمندان شدند. آرش فهمید که هر کار کوچیکی، هر قدمی که به سمت بهتر شدنِ چیزها برمی‌ده، می‌تونه یه جورایی به ستاره‌ها یه جور حسِ خوب بده.\n",
            "\n",
            "حالا، وقتی به آسمونِ تهران نگاه می‌کرد، نه فقط ستاره‌ها رو می‌دید، بلکه یه جورایی، یه نقاشیِ رنگارنگ از زندگیِ خودش و از شهرش رو می‌دید.\n",
            "\n",
            "---\n",
            "\n",
            "**Notes and Considerations:**\n",
            "\n",
            "*   **Tone:** I've aimed for a slightly melancholic, but ultimately hopeful tone, reflecting the complexity of living in a big city.\n",
            "*   **Details:** I’ve included specific details like \"Coffee Delency\" (coffee shop name) and \"Laleh Park\" to ground the story in a Tehran setting.\n",
            "*   **Theme:** The core theme is about finding purpose and making a difference, even in small ways.\n",
            "*   **Imagery:** I've focused on visual imagery (smoke, stars, paintings) to create a stronger sense of atmosphere.\n",
            "\n",
            "Would you like me to:\n",
            "\n",
            "*   Expand on a specific part of the story?\n",
            "*   Change the tone or focus?\n",
            "*   Add a particular element (e.g., a romantic subplot, a challenge he faces)?خداوند، به طور کلی، اشاره به خدای یکتا در ادیان ابراهیمی (یهودیت، مسیحیت و اسلام) است. این کلمه به معنای \"خداوند\" یا \"آفریننده\" است و به جای خدا (که نامی برای خداست) به عنوان یک عنوان یا نام‌گذاری برای او استفاده می‌شود.\n",
            "\n",
            "به طور خلاصه، خداوند به معنای **خدا** است، خدای یکتا و خالق جهان.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'خداوند، به طور کلی، اشاره به خدای یکتا در ادیان ابراهیمی (یهودیت، مسیحیت و اسلام) است. این کلمه به معنای \"خداوند\" یا \"آفریننده\" است و به جای خدا (که نامی برای خداست) به عنوان یک عنوان یا نام\\u200cگذاری برای او استفاده می\\u200cشود.\\n\\nبه طور خلاصه، خداوند به معنای **خدا** است، خدای یکتا و خالق جهان.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama ps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-SravhUP9th",
        "outputId": "c5b0917a-5893-4ea0-8dd9-18537afcee75"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME    ID    SIZE    PROCESSOR    UNTIL \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama stop llama3.2:latest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r-VQpg0QJ-q",
        "outputId": "c0544ddf-5616-4928-8d79-bb8e3dfe0dbb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama stop mistral"
      ],
      "metadata": {
        "id": "SWhIs1vfY45s",
        "outputId": "9c4dee4b-1d0b-456a-88bf-35d19c4f015c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama stop llama3.1:8b"
      ],
      "metadata": {
        "id": "_f7K8X0epyy4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}